{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ads_test\\.conda\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0   0.374540   0.185133   0.261706   0.672703   0.571996   0.393636   \n",
      "1   0.950714   0.541901   0.246979   0.796681   0.805432   0.473436   \n",
      "2   0.731994   0.872946   0.906255   0.250468   0.760161   0.854547   \n",
      "3   0.598658   0.732225   0.249546   0.624874   0.153900   0.340004   \n",
      "4   0.156019   0.806561   0.271950   0.571746   0.149249   0.869650   \n",
      "\n",
      "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  Target  \n",
      "0   0.648257   0.038799   0.720268    0.913578    0.373641    0.533031       1  \n",
      "1   0.172386   0.186773   0.687283    0.525360    0.332912    0.137899       1  \n",
      "2   0.872395   0.831246   0.095754    0.724910    0.176154    0.591243       0  \n",
      "3   0.613116   0.766768   0.922572    0.436048    0.607267    0.314786       0  \n",
      "4   0.157204   0.350643   0.568472    0.630035    0.476624    0.052349       0  \n",
      "Số lượng mẫu: 1000, Số lượng cột: 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "data = {}\n",
    "for i in range(1, 13):\n",
    "    col_name = f\"Feature_{i}\"\n",
    "    data[col_name] = np.random.rand(num_samples)  \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Target'] = np.random.randint(0, 2, size=num_samples) \n",
    "\n",
    "print(df.head())\n",
    "feature_columns = list(df.columns[:-1])  \n",
    "\n",
    "print(f\"Số lượng mẫu: {len(df)}, Số lượng cột: {len(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None,\n",
    "                 virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(GLUBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
    "                                               use_bias=False)\n",
    "        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                            momentum=self.momentum)\n",
    "        \n",
    "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
    "                                             use_bias=False)\n",
    "        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                          momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        output = self.bn_outout(self.fc_outout(inputs), \n",
    "                                training=training)\n",
    "        gate = self.bn_gate(self.fc_gate(inputs), \n",
    "                            training=training)\n",
    "    \n",
    "        return output * tf.keras.activations.sigmoid(gate) # GLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] = 0.02, skip=False):\n",
    "        super(FeatureTransformerBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.skip = skip\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.initial = GLUBlock(units = self.units, \n",
    "                                virtual_batch_size=self.virtual_batch_size, \n",
    "                                momentum=self.momentum)\n",
    "        self.residual =  GLUBlock(units = self.units, \n",
    "                                  virtual_batch_size=self.virtual_batch_size, \n",
    "                                  momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        initial = self.initial(inputs, training=training)\n",
    "        \n",
    "        if self.skip == True:\n",
    "            initial += inputs\n",
    "\n",
    "        residual = self.residual(initial, training=training) # skip\n",
    "        \n",
    "        return (initial + residual) * np.sqrt(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(self.units, \n",
    "                                        use_bias=False)\n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
    "        feature = self.bn(self.fc(inputs), \n",
    "                          training=training)\n",
    "        if priors is None:\n",
    "            output = feature\n",
    "        else:\n",
    "            output = feature * priors\n",
    "        \n",
    "        return tfa.activations.sparsemax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetStep, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.unique = FeatureTransformerBlock(units = self.units, \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum,\n",
    "                                              skip=True)\n",
    "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
    "        split = self.unique(shared, training=training)\n",
    "        keys = self.attention(split, priors, training=training)\n",
    "        masked = keys * inputs\n",
    "        \n",
    "        return split, masked, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int =1, \n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 outputs: int = 1, \n",
    "                 gamma: float = 1.3,\n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):            \n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)        \n",
    "        self.initial_step = TabNetStep(units = self.n_features, \n",
    "                                       virtual_batch_size=self.virtual_batch_size, \n",
    "                                       momentum=self.momentum)\n",
    "        self.steps = [TabNetStep(units = self.n_features, \n",
    "                                 virtual_batch_size=self.virtual_batch_size, \n",
    "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.final = tf.keras.layers.Dense(units = self.units, \n",
    "                                           use_bias=False)\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
    "        entropy_loss = 0.\n",
    "        encoded = 0.\n",
    "        output = 0.\n",
    "        importance = 0.\n",
    "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
    "        \n",
    "        B = prior * self.bn(X, training=training)\n",
    "        shared = self.shared_block(B, training=training)\n",
    "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
    "\n",
    "        for step in self.steps:\n",
    "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
    "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
    "            importance += keys\n",
    "            \n",
    "            shared = self.shared_block(masked, training=training)\n",
    "            split, masked, keys = step(B, shared, prior, training=training)\n",
    "            features = tf.keras.activations.relu(split)\n",
    "            \n",
    "            output += features\n",
    "            encoded += split\n",
    "            \n",
    "        self.add_loss(self.sparsity * entropy_loss)\n",
    "          \n",
    "        prediction = self.final(output)\n",
    "        return prediction, encoded, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=1, \n",
    "                 n_steps = 3, \n",
    "                 n_features = 8,\n",
    "                 outputs = 1, \n",
    "                 gamma = 1.3,\n",
    "                 epsilon = 1e-8, \n",
    "                 sparsity = 1e-5, \n",
    "                 virtual_batch_size=128, \n",
    "                 momentum=0.02):\n",
    "        super(TabNetDecoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)\n",
    "        self.steps = [FeatureTransformerBlock(units = self.n_features,\n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.fc = [tf.keras.layers.Dense(units = self.units) for _ in range(self.n_steps)]\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        decoded = 0.\n",
    "        \n",
    "        for ftb, fc in zip(self.steps, self.fc):\n",
    "            shared = self.shared_block(X, training=training)\n",
    "            feature = ftb(shared, training=training)\n",
    "            output = fc(feature)\n",
    "            \n",
    "            decoded += output\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/dense_24/kernel:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/gamma:0', 'tab_net_autoencoder_9/tab_net_encoder_9/tab_net_step_3/attentive_transformer_3/batch_normalization_24/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "4/4 [==============================] - 16s 7ms/step - loss: 1.3309\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1500\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0945\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1289\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0723\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8916\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9566\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9176\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8996\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9487\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9237\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9244\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9189\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9490\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8875\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8386\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8720\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8661\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8158\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7744\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8313\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8237\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8305\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7962\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8351\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8519\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8681\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8464\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8022\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8338\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7696\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7818\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7472\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7785\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7503\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7793\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7437\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7595\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7410\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7422\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7672\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7555\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7304\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7490\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7687\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7561\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7704\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7368\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7466\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7595\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7321\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7409\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7377\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7369\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7415\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7273\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7402\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7583\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7364\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7404\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7327\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7435\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7191\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7182\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7183\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7285\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7219\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7302\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7161\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7307\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7237\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7245\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7120\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7116\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7090\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7155\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7083\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7153\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7208\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7199\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7112\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7150\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7059\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7088\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7095\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7043\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7093\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7056\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7055\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7057\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7129\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7068\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7044\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7143\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7075\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7114\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7152\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7027\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6992\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7104\n",
      "Model: \"tab_net_autoencoder_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_8 (Lambda)           multiple                  0         \n",
      "                                                                 \n",
      " tab_net_encoder_9 (TabNetE  multiple                  618       \n",
      " ncoder)                                                         \n",
      "                                                                 \n",
      " tab_net_decoder_9 (TabNetD  multiple                  300       \n",
      " ecoder)                                                         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  multiple                  48        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 966 (3.77 KB)\n",
      "Trainable params: 678 (2.65 KB)\n",
      "Non-trainable params: 288 (1.12 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "class TabNetAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, outputs: int = 1, \n",
    "                 inputs: int = 12,\n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 gamma: float = 1.3, \n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
    "                 virtual_batch_size: Optional[int] = None, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(TabNetAutoencoder, self).__init__()\n",
    "        \n",
    "        self.outputs = outputs\n",
    "        self.inputs = inputs\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.feature_column = feature_column\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "        if feature_column is None:\n",
    "            self.feature = tf.keras.layers.Lambda(lambda x: x)\n",
    "        else:\n",
    "            self.feature = feature_column\n",
    "            \n",
    "        self.encoder = TabNetEncoder(units=outputs, \n",
    "                                    n_steps=n_steps, \n",
    "                                    n_features=n_features,\n",
    "                                    gamma=gamma, \n",
    "                                    epsilon=epsilon, \n",
    "                                    sparsity=sparsity,\n",
    "                                    virtual_batch_size=virtual_batch_size, \n",
    "                                    momentum=momentum)\n",
    "        \n",
    "        self.decoder = TabNetDecoder(units=inputs, \n",
    "                                     n_steps=n_steps, \n",
    "                                     n_features=n_features,\n",
    "                                     virtual_batch_size=virtual_batch_size, \n",
    "                                     momentum=momentum)\n",
    "        \n",
    "        self.bn = tf.keras.layers.BatchNormalization(momentum=momentum)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        X = self.bn(X, training=training)\n",
    "        \n",
    "        M = self.do(tf.ones_like(X), training=training)\n",
    "        D = X * M\n",
    "        \n",
    "        output, encoded, importance = self.encoder(D)\n",
    "        prediction = tf.keras.activations.sigmoid(output)        \n",
    "        \n",
    "        T = X * (1 - M)\n",
    "        reconstruction = self.decoder(encoded)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.where(M != 0., tf.square(T - reconstruction), tf.zeros_like(reconstruction)))\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        _, encoded, _, _, _ = self.encoder(X)\n",
    "        return encoded\n",
    "    \n",
    "    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        X = self.feature(X)\n",
    "        _, _, importance, _, _ = self.encoder(X)\n",
    "        return importance\n",
    "\n",
    "feature_column = None  \n",
    "\n",
    "ae = TabNetAutoencoder(outputs=1, inputs=12, n_steps=3, n_features=2, feature_column=feature_column, virtual_batch_size=None)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def dummy_loss(y, t):\n",
    "    return 0.\n",
    "\n",
    "ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='binary_crossentropy')\n",
    "\n",
    "X_train = np.random.randn(100, 12)\n",
    "y_train = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "ae.fit(X_train,y_train, epochs=100)\n",
    "ae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TabNetAutoencoder(tf.keras.Model):\n",
    "#     def __init__(self, outputs: int = 1, \n",
    "#                  inputs: int = 12,\n",
    "#                  n_steps: int  = 3, \n",
    "#                  n_features: int  = 8,\n",
    "#                  gamma: float = 1.3, \n",
    "#                  epsilon: float = 1e-8, \n",
    "#                  sparsity: float = 1e-5, \n",
    "#                  feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
    "#                  virtual_batch_size: Optional[int] = 128, \n",
    "#                  momentum: Optional[float] = 0.02):\n",
    "#         super(TabNetAutoencoder, self).__init__()\n",
    "        \n",
    "#         self.outputs = outputs\n",
    "#         self.inputs = inputs\n",
    "#         self.n_steps = n_steps\n",
    "#         self.n_features = n_features\n",
    "#         self.feature_column = feature_column\n",
    "#         self.virtual_batch_size = virtual_batch_size\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.momentum = momentum\n",
    "#         self.sparsity = sparsity\n",
    "        \n",
    "#         if feature_column is None:\n",
    "#             self.feature = tf.keras.layers.Lambda(identity)\n",
    "#         else:\n",
    "#             self.feature = feature_column\n",
    "            \n",
    "#         self.encoder = TabNetEncoder(units=outputs, \n",
    "#                                     n_steps=n_steps, \n",
    "#                                     n_features = n_features,\n",
    "#                                     outputs=outputs, \n",
    "#                                     gamma=gamma, \n",
    "#                                     epsilon=epsilon, \n",
    "#                                     sparsity=sparsity,\n",
    "#                                     virtual_batch_size=self.virtual_batch_size, \n",
    "#                                     momentum=momentum)\n",
    "        \n",
    "#         self.decoder = TabNetDecoder(units=inputs, \n",
    "#                                      n_steps=n_steps, \n",
    "#                                      n_features = n_features,\n",
    "#                                      virtual_batch_size=self.virtual_batch_size, \n",
    "#                                      momentum=momentum)\n",
    "        \n",
    "#         self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "#                                                      momentum=momentum)\n",
    "        \n",
    "#         self.do = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "#     def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n",
    "#         X = self.feature(X)\n",
    "#         X = self.bn(X)\n",
    "        \n",
    "#         # training mask\n",
    "#         M = self.do(tf.ones_like(X), training=training)\n",
    "#         D = X*M\n",
    "        \n",
    "#         #encoder\n",
    "#         output, encoded, importance = self.encoder(D)\n",
    "#         prediction = tf.keras.activations.sigmoid(output)        \n",
    "        \n",
    "#         return prediction, encoded, importance, X, M\n",
    "    \n",
    "#     def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         # encode\n",
    "#         prediction, encoded, _, X, M = self.forward(X)\n",
    "#         T = X * (1 - M)\n",
    "\n",
    "#         #decode\n",
    "#         reconstruction = self.decoder(encoded)\n",
    "        \n",
    "#         #loss\n",
    "#         loss  = tf.reduce_mean(tf.where(M != 0., tf.square(T-reconstruction), tf.zeros_like(reconstruction)))\n",
    "        \n",
    "#         self.add_loss(loss)\n",
    "        \n",
    "#         return prediction\n",
    "    \n",
    "#     def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         _, encoded, _, _, _ = self.forward(X)\n",
    "#         return encoded\n",
    "    \n",
    "#     def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "#         _, _, importance, _, _ = self.forward(X)\n",
    "#         return importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
