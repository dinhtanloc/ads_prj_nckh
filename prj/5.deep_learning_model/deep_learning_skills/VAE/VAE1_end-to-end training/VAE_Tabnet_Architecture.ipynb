{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# import tensorflow_probability as tfp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0   0.374540   0.185133   0.261706   0.672703   0.571996   0.393636   \n",
      "1   0.950714   0.541901   0.246979   0.796681   0.805432   0.473436   \n",
      "2   0.731994   0.872946   0.906255   0.250468   0.760161   0.854547   \n",
      "3   0.598658   0.732225   0.249546   0.624874   0.153900   0.340004   \n",
      "4   0.156019   0.806561   0.271950   0.571746   0.149249   0.869650   \n",
      "\n",
      "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  Target  \n",
      "0   0.648257   0.038799   0.720268    0.913578    0.373641    0.533031       1  \n",
      "1   0.172386   0.186773   0.687283    0.525360    0.332912    0.137899       1  \n",
      "2   0.872395   0.831246   0.095754    0.724910    0.176154    0.591243       0  \n",
      "3   0.613116   0.766768   0.922572    0.436048    0.607267    0.314786       0  \n",
      "4   0.157204   0.350643   0.568472    0.630035    0.476624    0.052349       0  \n",
      "Số lượng mẫu: 1000, Số lượng cột: 13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  \n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "data = {}\n",
    "for i in range(1, 13):\n",
    "    col_name = f\"Feature_{i}\"\n",
    "    data[col_name] = np.random.rand(num_samples)  \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Target'] = np.random.randint(0, 2, size=num_samples) \n",
    "\n",
    "print(df.head())\n",
    "feature_columns = list(df.columns[:-1])  \n",
    "\n",
    "print(f\"Số lượng mẫu: {len(df)}, Số lượng cột: {len(df.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None,\n",
    "                 virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(GLUBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
    "                                               use_bias=False)\n",
    "        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                            momentum=self.momentum)\n",
    "        \n",
    "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
    "                                             use_bias=False)\n",
    "        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                          momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        output = self.bn_outout(self.fc_outout(inputs), \n",
    "                                training=training)\n",
    "        gate = self.bn_gate(self.fc_gate(inputs), \n",
    "                            training=training)\n",
    "    \n",
    "        return output * tf.keras.activations.sigmoid(gate) # GLU\n",
    "    \n",
    "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] = 0.02, skip=False):\n",
    "        super(FeatureTransformerBlock, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.skip = skip\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.initial = GLUBlock(units = self.units, \n",
    "                                virtual_batch_size=self.virtual_batch_size, \n",
    "                                momentum=self.momentum)\n",
    "        self.residual =  GLUBlock(units = self.units, \n",
    "                                  virtual_batch_size=self.virtual_batch_size, \n",
    "                                  momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
    "        initial = self.initial(inputs, training=training)\n",
    "        \n",
    "        if self.skip == True:\n",
    "            initial += inputs\n",
    "\n",
    "        residual = self.residual(initial, training=training) # skip\n",
    "        \n",
    "        return (initial + residual) * np.sqrt(0.5)\n",
    "\n",
    "class AttentiveTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n",
    "                 momentum: Optional[float] = 0.02):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(self.units, \n",
    "                                        use_bias=False)\n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
    "        feature = self.bn(self.fc(inputs), \n",
    "                          training=training)\n",
    "        if priors is None:\n",
    "            output = feature\n",
    "        else:\n",
    "            output = feature * priors\n",
    "        \n",
    "        return tfa.activations.sparsemax(output)\n",
    "\n",
    "class TabNetStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetStep, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.unique = FeatureTransformerBlock(units = self.units, \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum,\n",
    "                                              skip=True)\n",
    "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum)\n",
    "        \n",
    "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
    "        split = self.unique(shared, training=training)\n",
    "        keys = self.attention(split, priors, training=training)\n",
    "        masked = keys * inputs\n",
    "        \n",
    "        return split, masked, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int =1, \n",
    "                 n_steps: int = 3, \n",
    "                 n_features: int = 8,\n",
    "                 outputs: int = 1, \n",
    "                 gamma: float = 1.3,\n",
    "                 epsilon: float = 1e-8, \n",
    "                 sparsity: float = 1e-5, \n",
    "                 virtual_batch_size: Optional[int]=128, \n",
    "                 momentum: Optional[float] =0.02):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):            \n",
    "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
    "                                                     momentum=self.momentum)\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)        \n",
    "        self.initial_step = TabNetStep(units = self.n_features, \n",
    "                                       virtual_batch_size=self.virtual_batch_size, \n",
    "                                       momentum=self.momentum)\n",
    "        self.steps = [TabNetStep(units = self.n_features, \n",
    "                                 virtual_batch_size=self.virtual_batch_size, \n",
    "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.final = tf.keras.layers.Dense(units = self.units, \n",
    "                                           use_bias=False)\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
    "        entropy_loss = 0.\n",
    "        encoded = 0.\n",
    "        output = 0.\n",
    "        importance = 0.\n",
    "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
    "        \n",
    "        B = prior * self.bn(X, training=training)\n",
    "        shared = self.shared_block(B, training=training)\n",
    "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
    "\n",
    "        for step in self.steps:\n",
    "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
    "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
    "            importance += keys\n",
    "            \n",
    "            shared = self.shared_block(masked, training=training)\n",
    "            split, masked, keys = step(B, shared, prior, training=training)\n",
    "            features = tf.keras.activations.relu(split)\n",
    "            \n",
    "            output += features\n",
    "            encoded += split\n",
    "            \n",
    "        self.add_loss(self.sparsity * entropy_loss)\n",
    "          \n",
    "        prediction = self.final(output)\n",
    "        return prediction, encoded, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=1, \n",
    "                 n_steps = 3, \n",
    "                 n_features = 8,\n",
    "                 outputs = 1, \n",
    "                 gamma = 1.3,\n",
    "                 epsilon = 1e-8, \n",
    "                 sparsity = 1e-5, \n",
    "                 virtual_batch_size=128, \n",
    "                 momentum=0.02):\n",
    "        super(TabNetDecoder, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
    "                                                    virtual_batch_size=self.virtual_batch_size, \n",
    "                                                    momentum=self.momentum)\n",
    "        self.steps = [FeatureTransformerBlock(units = self.n_features,\n",
    "                                              virtual_batch_size=self.virtual_batch_size, \n",
    "                                              momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "        self.fc = [tf.keras.layers.Dense(units = self.units) for _ in range(self.n_steps)]\n",
    "    \n",
    "\n",
    "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
    "        decoded = 0.\n",
    "        \n",
    "        for ftb, fc in zip(self.steps, self.fc):\n",
    "            shared = self.shared_block(X, training=training)\n",
    "            feature = ftb(shared, training=training)\n",
    "            output = fc(feature)\n",
    "            \n",
    "            decoded += output\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = ops.shape(z_mean)[0]\n",
    "        dim = ops.shape(z_mean)[1]\n",
    "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, tabnet_encoder, mlp_units, latent_dim=64, name='vae_encoder', **kwargs):\n",
    "        super(VAE_Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.tabnet_encoder = tabnet_encoder\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation='relu') for units in mlp_units\n",
    "        ])\n",
    "        self.mean_layer = tf.keras.layers.Dense(latent_dim)\n",
    "        self.logvar_layer = tf.keras.layers.Dense(latent_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        _, encoded, _ = self.tabnet_encoder(inputs, training=training)\n",
    "        encoded = self.mlp(encoded, training=training)\n",
    "        z_mean = self.mean_layer(encoded)\n",
    "        z_log_var = self.logvar_layer(encoded)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var,z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, mlp_units, tabnet_decoder, name='vae_decoder', **kwargs):\n",
    "        super(VAE_Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation='relu') for units in reversed(mlp_units)\n",
    "        ])\n",
    "        self.tabnet_decoder = tabnet_decoder\n",
    "\n",
    "    def call(self, inputs=64, training=False):\n",
    "        decoded = self.mlp(inputs, training=training)\n",
    "        reconstructed = self.tabnet_decoder(decoded, training=training)\n",
    "        return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Tabnet_MLPS(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.classification_loss_tracker = keras.metrics.Mean(name=\"classification_loss\")\n",
    "        self.accuracy_tracker = keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x_train, y_train = data \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x_train)\n",
    "            print(z.shape)\n",
    "            reconstruction = self.decoder(z)\n",
    "            classification_output = self.classifier(z)\n",
    "            \n",
    "            reconstruction_loss = keras.losses.binary_crossentropy(x_train, reconstruction)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(reconstruction_loss, axis=1))\n",
    "\n",
    "            # accuracy = self.accuracy_tracker(y_train, classification_output)\n",
    "\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            print(classification_output.shape,y_train.shape)\n",
    "            \n",
    "            classification_loss = keras.losses.binary_crossentropy(y_train, classification_output)\n",
    "            print('hello')\n",
    "            classification_loss = tf.reduce_mean(classification_loss)\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss + classification_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.classification_loss_tracker.update_state(classification_loss)\n",
    "        self.accuracy_tracker.update_state(y_train, classification_output)\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"classification_loss\": self.classification_loss_tracker.result(),\n",
    "            \"accuracy\": self.accuracy_tracker.result()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo TabNetEncoder và TabNetDecoder\n",
    "latent_dim=64\n",
    "tabnet_encoder = TabNetEncoder(units=latent_dim, n_steps=3, n_features=12)\n",
    "tabnet_decoder = TabNetDecoder(units=12, n_steps=3, n_features=12)\n",
    "\n",
    "# Khởi tạo VAE\n",
    "vae_encoder = VAE_Encoder(tabnet_encoder, mlp_units=[64, 32], latent_dim=16)\n",
    "vae_decoder = VAE_Decoder(mlp_units=[32, 64], tabnet_decoder=tabnet_decoder)\n",
    "vae = VAE(encoder=vae_encoder, decoder=vae_decoder)\n",
    "\n",
    "# Compile và huấn luyện VAE\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "vae.fit(df, epochs=50)\n",
    "\n",
    "# Sử dụng mô hình để dự đoán và tái tạo dữ liệu\n",
    "reconstructed_data = vae.predict(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
