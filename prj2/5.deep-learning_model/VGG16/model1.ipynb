{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ads_test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.29.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys,os\n",
    "__script_path=os.path.abspath(globals().get('__file__','.'))\n",
    "__script_dir = os.path.dirname(__script_path)\n",
    "root_dir = os.path.abspath(f'{__script_dir}/../..')\n",
    "print(root_dir)\n",
    "for lib in [root_dir][::-1]:\n",
    "    if lib in sys.path:\n",
    "        sys.path.remove(lib)\n",
    "    sys.path.insert(0,lib)\n",
    "from libs.lib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_dir = \"../../exps/exp1\"\n",
    "if os.path.exists(exps_dir) == False: # tạo thư mục (nếu chưa có)\n",
    "  os.makedirs(exps_dir, exist_ok=True)\n",
    "\n",
    "save_dir = f\"{exps_dir}/data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "test_size=0.33\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinhtanloc\u001b[0m (\u001b[33mnckh_ueh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b63f39d5ee143a6af11538aa86d1b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ads_test\\prj2\\5.deep-learning_model\\VGG16\\wandb\\run-20240423_143134-10nwljwk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/10nwljwk' target=\"_blank\">comfy-wave-11</a></strong> to <a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16' target=\"_blank\">https://wandb.ai/nckh_ueh/ads1_prj_VGG16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/10nwljwk' target=\"_blank\">https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/10nwljwk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/10nwljwk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x25c794d5040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ads1_prj_VGG16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../exps/exp1/data/x_train.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/x_train.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m y_train\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/y_train.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m x_test\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/x_test.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\ads_test\\.conda\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../exps/exp1/data/x_train.xlsx'"
     ]
    }
   ],
   "source": [
    "x_train=pd.read_excel(f'{save_dir}/x_train.xlsx')\n",
    "y_train=pd.read_excel(f'{save_dir}/y_train.xlsx')\n",
    "x_test=pd.read_excel(f'{save_dir}/x_test.xlsx')\n",
    "y_test=pd.read_excel(f'{save_dir}/y_test.xlsx')\n",
    "class_weights_dict=dict(np.load(f'{save_dir}/class_weights_dict.npz',allow_pickle=True))['class_weights_dict']\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "class_weights_dict = {key: value for key, value in class_weights_dict.item().items()}\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "def f1_loss(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true, tf.float32))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return 1 - f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "    loss = -tf.reduce_mean(alpha * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    return 1 - (numerator + 1) / (denominator + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred, smooth=0.0001):\n",
    "    return 1 - dice_coef(y_true, y_pred, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    F1 score metric.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "input_shape =(x_train.shape[1],1)\n",
    "x_input = Input(shape=input_shape)\n",
    "dense_units = 1024\n",
    "learning_rate = 0.0001\n",
    "kernel_size =3\n",
    "filter_size=96\n",
    "\n",
    "\n",
    "# Block 1\n",
    "x = Conv1D(filter_size, kernel_size, activation='relu', padding='same', name='block1_conv1')(x_input)\n",
    "x = Conv1D(filter_size, kernel_size, activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv1D(filter_size*2, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv1D(filter_size*2, kernel_size, activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block2_pool')(x)\n",
    "\n",
    "# Block 3\n",
    "x = Conv1D(filter_size*4, kernel_size, activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = Conv1D(filter_size*4, kernel_size, activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = Conv1D(filter_size*4, kernel_size, activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block3_pool')(x)\n",
    "\n",
    "# Block 4\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block4_pool')(x)\n",
    "\n",
    "# Block 5\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block5_pool')(x)\n",
    "\n",
    "# x = Flatten(name='flatten')(x)\n",
    "# x = Dense(dense_units, activation='relu', name='fc1')(x)\n",
    "# x = Dense(dense_units, activation='relu', name='fc2')(x)\n",
    "# x = Dense(1, activation='sigmoid', name='predictions')(x)  # Sigmoid for binary classification\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "# x = GlobalMaxPooling1D()(x)\n",
    "# x = Dense(2, activation='softmax', name='predictions')(x)  # Sigmoid for binary classification\n",
    "x = Dense(1, activation='sigmoid', name='predictions')(x)  # Sigmoid for binary classification\n",
    "\n",
    "# Create model.\n",
    "model = Model(x_input, x, name='vgg16')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='binary_crossentropy', metrics=['accuracy',f1_score])\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy',f1_score])\n",
    "# model.compile(optimizer=Adam(lr=learning_rate), loss=f1_loss, metrics=['accuracy',f1_score])\n",
    "# model.compile(optimizer=Adam(lr=learning_rate), loss=focal_loss, metrics=['accuracy',f1_score])\n",
    "# model.compile(optimizer=Adam(lr=learning_rate), loss=dice_coef_loss, metrics=['accuracy',f1_score])\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "    # min_delta=0.00005,\n",
    "    patience=60,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.95,\n",
    "    patience=10,\n",
    "    min_lr=0.000001,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 34, 1)]           0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv1D)       (None, 34, 96)            384       \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 34, 96)            27744     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 17, 96)            0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 17, 192)           55488     \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 17, 192)           110784    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 8, 192)            0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 8, 384)            221568    \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 8, 384)            442752    \n",
      "                                                                 \n",
      " block3_conv3 (Conv1D)       (None, 8, 384)            442752    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 4, 384)            0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv1D)       (None, 4, 768)            885504    \n",
      "                                                                 \n",
      " block4_conv2 (Conv1D)       (None, 4, 768)            1770240   \n",
      "                                                                 \n",
      " block4_conv3 (Conv1D)       (None, 4, 768)            1770240   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling1D)  (None, 2, 768)            0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv1D)       (None, 2, 768)            1770240   \n",
      "                                                                 \n",
      " block5_conv2 (Conv1D)       (None, 2, 768)            1770240   \n",
      "                                                                 \n",
      " block5_conv3 (Conv1D)       (None, 2, 768)            1770240   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling1D)  (None, 1, 768)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 768)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11038945 (42.11 MB)\n",
      "Trainable params: 11038945 (42.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pic(times):\n",
    "    history=model.history.history\n",
    "    fig,axes=plt.subplots(1,2, figsize=(15,8))\n",
    "    axes[0].plot(pd.DataFrame(history['val_accuracy']))\n",
    "    axes[0].set_title('Training Process')\n",
    "\n",
    "    axes[1].plot(pd.DataFrame(history['accuracy']), label='Train Accuracy')\n",
    "    axes[1].plot(pd.DataFrame(history['val_accuracy']), label='Validation Accuracy')\n",
    "    axes[1].plot(pd.DataFrame(history['loss']), label='Train Loss')\n",
    "    axes[1].plot(pd.DataFrame(history['val_loss']), label='Validation Loss')\n",
    "    axes[1].set_title(f'Training Process in {times}')\n",
    "    axes[1].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 58s 385ms/step - loss: 0.2548 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2460 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "146/146 [==============================] - 53s 362ms/step - loss: 0.2331 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2496 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 54s 372ms/step - loss: 0.2269 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2355 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 54s 368ms/step - loss: 0.2147 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2322 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 53s 360ms/step - loss: 0.2069 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2124 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 52s 354ms/step - loss: 0.2002 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2080 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 55s 377ms/step - loss: 0.1949 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2033 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "146/146 [==============================] - 51s 347ms/step - loss: 0.1936 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2250 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "146/146 [==============================] - 51s 353ms/step - loss: 0.1905 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2067 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "146/146 [==============================] - 52s 354ms/step - loss: 0.1878 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2059 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "131/146 [=========================>....] - ETA: 5s - loss: 0.1853 - accuracy: 0.9412 - f1_score: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# Số lượng folds\n",
    "n_splits = 10\n",
    "\n",
    "# KFold\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "times=0\n",
    "\n",
    "# Tạo và đào tạo mô hình với K-Fold\n",
    "# Tạo và đào tạo mô hình với K-Fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "    times+=1\n",
    "    X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    # Chuyển đổi tf.Tensor thành mảng NumPy nếu cần thiết\n",
    "    X_train_fold = X_train_fold.numpy() if isinstance(X_train_fold, tf.Tensor) else X_train_fold\n",
    "    y_train_fold = y_train_fold.numpy() if isinstance(y_train_fold, tf.Tensor) else y_train_fold\n",
    "    X_val_fold = X_val_fold.numpy() if isinstance(X_val_fold, tf.Tensor) else X_val_fold\n",
    "    y_val_fold = y_val_fold.numpy() if isinstance(y_val_fold, tf.Tensor) else y_val_fold\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=1000, batch_size=64, validation_data=(X_val_fold, y_val_fold),callbacks=[wandb.keras.WandbCallback(),early_stopping,lr_scheduler])\n",
    "    show_pic(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lấy dữ liệu từ history\n",
    "history = model.history.history\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Biểu đồ accuracy của tập kiểm tra và tập validation\n",
    "axes[0].plot(pd.DataFrame(history['accuracy']), label='Train Accuracy')\n",
    "axes[0].plot(pd.DataFrame(history['val_accuracy']), label='Validation Accuracy')\n",
    "axes[0].set_title('Training Process')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "\n",
    "# Biểu đồ loss của tập kiểm tra và tập validation\n",
    "axes[1].plot(pd.DataFrame(history['loss']), label='Train Loss')\n",
    "axes[1].plot(pd.DataFrame(history['val_loss']), label='Validation Loss')\n",
    "axes[1].set_title('Training Process')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()\n",
    "\n",
    "# Lưu biểu đồ vào file\n",
    "plt.savefig('train.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Đánh giá độ chính xác\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hiển thị báo cáo phân loại\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions,labels=[1,0]))\n",
    "\n",
    "# # Hiển thị ma trận nhầm lẫn\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính ma trận nhầm lẫn\n",
    "cm = confusion_matrix(y_test, predictions,labels=[1,0])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=[1,0], yticklabels=[1,0])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_test, predictions,ax,name='model'):    \n",
    "    # Tính toán FPR và TPR từ decision function\n",
    "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "    \n",
    "    # Tính diện tích dưới đường cong ROC (AUC)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Vẽ đường cong ROC\n",
    "    ax.plot(fpr, tpr, lw=4, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12, weight='bold')  # Đặt kích thước và độ dày cho nhãn trục x\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12, weight='bold')   # Đặt kích thước và độ dày cho nhãn trục y\n",
    "    ax.set_title(f'ROC of {name}',fontsize=20, weight='bold')  # Đặt kích thước và độ dày cho tiêu đề\n",
    "    ax.legend(loc=\"lower right\", prop={'size': 12, 'weight': 'bold'})  # Đặt kích thước và độ dày cho chú thích\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "plot_roc_curve(y_test,predictions,axes)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_result(prediction):\n",
    "    prediction = (prediction > 0.5).astype(int)\n",
    "    if isinstance(prediction, (np.ndarray, pd.Series)):\n",
    "        prediction = prediction.item()  # Hoặc sử dụng prediction[0] hoặc prediction.iloc[0]\n",
    "    if prediction==1:\n",
    "        return \"Y\"\n",
    "    else:\n",
    "        return \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solandung=0\n",
    "hopdonggianlan=0\n",
    "for indx in range(len(x_test)):\n",
    "    y_predict = predictions\n",
    "    #print(y_predict[0][0])\n",
    "    Y_check = check_result(y_test.iloc[indx])\n",
    "    print('True:', Y_check)\n",
    "    print('Predicted:', check_result(y_predict[indx]))\n",
    "    print(\"----------------------------\")\n",
    "    if Y_check==check_result(y_predict[indx]):\n",
    "        solandung+=1\n",
    "        if Y_check=='Y':\n",
    "            hopdonggianlan+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Số lần dự đoán đúng: {solandung} chiếm khoảng {(solandung/len(x_test))*100}% tổng lần dự đoán\")\n",
    "print(f\"Số hợp đồng gian lận trong lần kiểm tra: {hopdonggianlan} chiếm khoảng {(hopdonggianlan/len(y_test[y_test['fraud_reported']==1]))*100}% so với thực tế\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
