{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ads_test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.29.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys,os\n",
    "__script_path=os.path.abspath(globals().get('__file__','.'))\n",
    "__script_dir = os.path.dirname(__script_path)\n",
    "root_dir = os.path.abspath(f'{__script_dir}/../..')\n",
    "print(root_dir)\n",
    "for lib in [root_dir][::-1]:\n",
    "    if lib in sys.path:\n",
    "        sys.path.remove(lib)\n",
    "    sys.path.insert(0,lib)\n",
    "from libs.lib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_dir = \"../../../exps/exp2\"\n",
    "if os.path.exists(exps_dir) == False: # tạo thư mục (nếu chưa có)\n",
    "  os.makedirs(exps_dir, exist_ok=True)\n",
    "\n",
    "save_dir = f\"{exps_dir}/data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdinhtanloc\u001b[0m (\u001b[33mnckh_ueh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/ij82azrm' target=\"_blank\">azure-feather-7</a></strong> to <a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16' target=\"_blank\">https://wandb.ai/nckh_ueh/ads1_prj_VGG16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/ij82azrm' target=\"_blank\">https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/ij82azrm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nckh_ueh/ads1_prj_VGG16/runs/ij82azrm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1bfa4f64250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ads1_prj_VGG16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu\n",
    "x_train = pd.read_excel(f'{save_dir}/x_train.xlsx')\n",
    "y_train = pd.read_excel(f'{save_dir}/y_train.xlsx')\n",
    "x_test = pd.read_excel(f'{save_dir}/x_test.xlsx')\n",
    "y_test = pd.read_excel(f'{save_dir}/y_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "def f1_loss(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true, tf.float32))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return 1 - f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "    loss = -tf.reduce_mean(alpha * tf.pow(1.0 - pt, gamma) * tf.math.log(pt))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    return 1 - (numerator + 1) / (denominator + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return dice\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred, smooth=0.0001):\n",
    "    return 1 - dice_coef(y_true, y_pred, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    F1 score metric.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "input_shape =(x_train.shape[1],1)\n",
    "x_input = Input(shape=input_shape)\n",
    "dense_units = 1024\n",
    "learning_rate = 0.0001\n",
    "kernel_size =3\n",
    "filter_size=96\n",
    "\n",
    "\n",
    "# Block 1\n",
    "x = Conv1D(filter_size, kernel_size, activation='relu', padding='same', name='block1_conv1')(x_input)\n",
    "x = Conv1D(filter_size, kernel_size, activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv1D(filter_size*2, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv1D(filter_size*2, kernel_size, activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block2_pool')(x)\n",
    "\n",
    "# Block 3\n",
    "x = Conv1D(filter_size*4, kernel_size, activation='relu', padding='same', name='block3_conv1')(x)\n",
    "x = Conv1D(filter_size*4, kernel_size, activation='relu', padding='same', name='block3_conv2')(x)\n",
    "x = Conv1D(filter_size*4, kernel_size, activation='relu', padding='same', name='block3_conv3')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block3_pool')(x)\n",
    "\n",
    "# Block 4\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block4_pool')(x)\n",
    "\n",
    "# Block 5\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = Conv1D(filter_size*8, kernel_size, activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = MaxPooling1D(2, strides=2, name='block5_pool')(x)\n",
    "\n",
    "# x = Flatten(name='flatten')(x)\n",
    "# x = Dense(dense_units, activation='relu', name='fc1')(x)\n",
    "# x = Dense(dense_units, activation='relu', name='fc2')(x)\n",
    "# x = Dense(1, activation='sigmoid', name='predictions')(x)  # Sigmoid for binary classification\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "# x = GlobalMaxPooling1D()(x)\n",
    "# x = Dense(2, activation='softmax', name='predictions')(x)  # Sigmoid for binary classification\n",
    "x = Dense(1, activation='sigmoid', name='predictions')(x)  # Sigmoid for binary classification\n",
    "\n",
    "# Create model.\n",
    "model = Model(x_input, x, name='vgg16')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='binary_crossentropy', metrics=['accuracy',f1_score])\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy',f1_score])\n",
    "# model.compile(optimizer=Adam(lr=learning_rate), loss=f1_loss, metrics=['accuracy',f1_score])\n",
    "# model.compile(optimizer=Adam(lr=learning_rate), loss=focal_loss, metrics=['accuracy',f1_score])\n",
    "# model.compile(optimizer=Adam(lr=learning_rate), loss=dice_coef_loss, metrics=['accuracy',f1_score])\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "    # min_delta=0.00005,\n",
    "    patience=60,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.95,\n",
    "    patience=10,\n",
    "    min_lr=0.000001,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 34, 1)]           0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv1D)       (None, 34, 96)            384       \n",
      "                                                                 \n",
      " block1_conv2 (Conv1D)       (None, 34, 96)            27744     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling1D)  (None, 17, 96)            0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv1D)       (None, 17, 192)           55488     \n",
      "                                                                 \n",
      " block2_conv2 (Conv1D)       (None, 17, 192)           110784    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling1D)  (None, 8, 192)            0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv1D)       (None, 8, 384)            221568    \n",
      "                                                                 \n",
      " block3_conv2 (Conv1D)       (None, 8, 384)            442752    \n",
      "                                                                 \n",
      " block3_conv3 (Conv1D)       (None, 8, 384)            442752    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling1D)  (None, 4, 384)            0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv1D)       (None, 4, 768)            885504    \n",
      "                                                                 \n",
      " block4_conv2 (Conv1D)       (None, 4, 768)            1770240   \n",
      "                                                                 \n",
      " block4_conv3 (Conv1D)       (None, 4, 768)            1770240   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling1D)  (None, 2, 768)            0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv1D)       (None, 2, 768)            1770240   \n",
      "                                                                 \n",
      " block5_conv2 (Conv1D)       (None, 2, 768)            1770240   \n",
      "                                                                 \n",
      " block5_conv3 (Conv1D)       (None, 2, 768)            1770240   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling1D)  (None, 1, 768)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 768)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1)                 769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11038945 (42.11 MB)\n",
      "Trainable params: 11038945 (42.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pic(times):\n",
    "    history=model.history.history\n",
    "    fig,axes=plt.subplots(1,2, figsize=(15,8))\n",
    "    axes[0].plot(pd.DataFrame(history['val_accuracy']))\n",
    "    axes[0].set_title('Training Process')\n",
    "\n",
    "    axes[1].plot(pd.DataFrame(history['accuracy']), label='Train Accuracy')\n",
    "    axes[1].plot(pd.DataFrame(history['val_accuracy']), label='Validation Accuracy')\n",
    "    axes[1].plot(pd.DataFrame(history['loss']), label='Train Loss')\n",
    "    axes[1].plot(pd.DataFrame(history['val_loss']), label='Validation Loss')\n",
    "    axes[1].set_title(f'Training Process in {times}')\n",
    "    axes[1].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 58s 385ms/step - loss: 0.2548 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2460 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "146/146 [==============================] - 53s 362ms/step - loss: 0.2331 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2496 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 54s 372ms/step - loss: 0.2269 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2355 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 54s 368ms/step - loss: 0.2147 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2322 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 53s 360ms/step - loss: 0.2069 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2124 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 52s 354ms/step - loss: 0.2002 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2080 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9412 - f1_score: 0.0000e+00INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (d:\\ads_test\\prj1\\5.deep_learning_model\\VGG16\\wandb\\run-20240328_235213-ij82azrm\\files\\model-best)... Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 55s 377ms/step - loss: 0.1949 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2033 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "146/146 [==============================] - 51s 347ms/step - loss: 0.1936 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2250 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "146/146 [==============================] - 51s 353ms/step - loss: 0.1905 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2067 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "146/146 [==============================] - 52s 354ms/step - loss: 0.1878 - accuracy: 0.9412 - f1_score: 0.0000e+00 - val_loss: 0.2059 - val_accuracy: 0.9333 - val_f1_score: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "131/146 [=========================>....] - ETA: 5s - loss: 0.1853 - accuracy: 0.9412 - f1_score: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# Số lượng folds\n",
    "n_splits = 10\n",
    "\n",
    "# KFold\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "times=0\n",
    "\n",
    "# Tạo và đào tạo mô hình với K-Fold\n",
    "# Tạo và đào tạo mô hình với K-Fold\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "    times+=1\n",
    "    X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    # Chuyển đổi tf.Tensor thành mảng NumPy nếu cần thiết\n",
    "    X_train_fold = X_train_fold.numpy() if isinstance(X_train_fold, tf.Tensor) else X_train_fold\n",
    "    y_train_fold = y_train_fold.numpy() if isinstance(y_train_fold, tf.Tensor) else y_train_fold\n",
    "    X_val_fold = X_val_fold.numpy() if isinstance(X_val_fold, tf.Tensor) else X_val_fold\n",
    "    y_val_fold = y_val_fold.numpy() if isinstance(y_val_fold, tf.Tensor) else y_val_fold\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=1000, batch_size=64, validation_data=(X_val_fold, y_val_fold),callbacks=[wandb.keras.WandbCallback(),early_stopping,lr_scheduler])\n",
    "    show_pic(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lấy dữ liệu từ history\n",
    "history = model.history.history\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# Biểu đồ accuracy của tập kiểm tra và tập validation\n",
    "axes[0].plot(pd.DataFrame(history['accuracy']), label='Train Accuracy')\n",
    "axes[0].plot(pd.DataFrame(history['val_accuracy']), label='Validation Accuracy')\n",
    "axes[0].set_title('Training Process')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "\n",
    "# Biểu đồ loss của tập kiểm tra và tập validation\n",
    "axes[1].plot(pd.DataFrame(history['loss']), label='Train Loss')\n",
    "axes[1].plot(pd.DataFrame(history['val_loss']), label='Validation Loss')\n",
    "axes[1].set_title('Training Process')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()\n",
    "\n",
    "# Lưu biểu đồ vào file\n",
    "plt.savefig('train.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Đánh giá độ chính xác\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hiển thị báo cáo phân loại\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions,labels=[1,0]))\n",
    "\n",
    "# # Hiển thị ma trận nhầm lẫn\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính ma trận nhầm lẫn\n",
    "cm = confusion_matrix(y_test, predictions,labels=[1,0])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=[1,0], yticklabels=[1,0])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_test, predictions,ax,name='model'):    \n",
    "    # Tính toán FPR và TPR từ decision function\n",
    "    fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "    \n",
    "    # Tính diện tích dưới đường cong ROC (AUC)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Vẽ đường cong ROC\n",
    "    ax.plot(fpr, tpr, lw=4, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12, weight='bold')  # Đặt kích thước và độ dày cho nhãn trục x\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12, weight='bold')   # Đặt kích thước và độ dày cho nhãn trục y\n",
    "    ax.set_title(f'ROC of {name}',fontsize=20, weight='bold')  # Đặt kích thước và độ dày cho tiêu đề\n",
    "    ax.legend(loc=\"lower right\", prop={'size': 12, 'weight': 'bold'})  # Đặt kích thước và độ dày cho chú thích\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "plot_roc_curve(y_test,predictions,axes)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_result(prediction):\n",
    "    prediction = (prediction > 0.5).astype(int)\n",
    "    if isinstance(prediction, (np.ndarray, pd.Series)):\n",
    "        prediction = prediction.item()  # Hoặc sử dụng prediction[0] hoặc prediction.iloc[0]\n",
    "    if prediction==1:\n",
    "        return \"Y\"\n",
    "    else:\n",
    "        return \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solandung=0\n",
    "hopdonggianlan=0\n",
    "for indx in range(len(x_test)):\n",
    "    y_predict = predictions\n",
    "    #print(y_predict[0][0])\n",
    "    Y_check = check_result(y_test.iloc[indx])\n",
    "    print('True:', Y_check)\n",
    "    print('Predicted:', check_result(y_predict[indx]))\n",
    "    print(\"----------------------------\")\n",
    "    if Y_check==check_result(y_predict[indx]):\n",
    "        solandung+=1\n",
    "        if Y_check=='Y':\n",
    "            hopdonggianlan+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Số lần dự đoán đúng: {solandung} chiếm khoảng {(solandung/len(x_test))*100}% tổng lần dự đoán\")\n",
    "print(f\"Số hợp đồng gian lận trong lần kiểm tra: {hopdonggianlan} chiếm khoảng {(hopdonggianlan/len(y_test[y_test['fraud_reported']==1]))*100}% so với thực tế\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
